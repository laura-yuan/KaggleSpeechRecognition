{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def triplet_loss(y_pred, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss as defined by formula (3)\n",
    "\n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor images, of shape (None, 128)\n",
    "            positive -- the encodings for the positive images, of shape (None, 128)\n",
    "            negative -- the encodings for the negative images, of shape (None, 128)\n",
    "\n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "\n",
    "    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]\n",
    "\n",
    "    ### START CODE HERE ### (â‰ˆ 4 lines)\n",
    "    # Step 1: Compute the (encoding) distance between the anchor and the positive, you will need to sum over axis=-1\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)\n",
    "    # Step 2: Compute the (encoding) distance between the anchor and the negative, you will need to sum over axis=-1\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)\n",
    "    # Step 3: subtract the two previous distances and add alpha.\n",
    "    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)\n",
    "    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.\n",
    "    loss = tf.reduce_sum(tf.nn.relu(basic_loss))\n",
    "    ### END CODE HERE ###\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_fully_connected_layer(X, output_count, layer_name, encoding_vector_name, reuse_flag):\n",
    "    with tf.name_scope(layer_name):\n",
    "        X_flatten = tf.layers.flatten(X, name='flatten')\n",
    "    W_shape_input = X_flatten.get_shape()\n",
    "    W_shape = [W_shape_input[1], output_count]\n",
    "    with tf.variable_scope(layer_name, reuse = reuse_flag):\n",
    "        W = tf.get_variable('weight', W_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B = tf.get_variable('biases', output_count, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    with tf.name_scope(layer_name + encoding_vector_name):\n",
    "        output = tf.add(tf.matmul(X_flatten, W), B)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_layer_full(X, dropout_prob, is_training_flag, F, F_stride, M, M_stride, layer_name,\n",
    "               is_batch_normalization_flag=False, is_pooling_flag=None,\n",
    "               nonlinear_act=tf.nn.relu, pooling_act=tf.nn.max_pool,\n",
    "               use_cudnn_on_gpu=False, reuse_convlayer_flag = False):\n",
    "    \"\"\"\n",
    "    Reusable code for making a simple neural net layer --  convolution part\n",
    "    It does a matrix multiply, bias add, and then uses relu to nonlinearize.\n",
    "    It also sets up name scoping so that the resultant graph is easy to read,\n",
    "    and adds a number of summary ops.\n",
    "\n",
    "    Arguments:\n",
    "    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    f -- integer, specifying the shape of the middle CONV's window [height, width, channel_prev, channel_curr]\n",
    "\n",
    "    Returns:\n",
    "    Y -- output of this layer, tensor of shape (m, n_H, n_W, n_C)\n",
    "\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "\n",
    "        # convolution\n",
    "        with tf.variable_scope(layer_name, reuse = reuse_convlayer_flag):\n",
    "            W = tf.get_variable('weight', F, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            B = tf.get_variable('bias', F[3], initializer=tf.zeros_initializer())\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        Z = tf.add(tf.nn.conv2d(X, W, strides=F_stride, padding='SAME', use_cudnn_on_gpu=use_cudnn_on_gpu), B,\n",
    "                   name='preactivation')\n",
    "\n",
    "        # batch normalization\n",
    "        # with tf.variable_scope(layer_name):\n",
    "        if is_batch_normalization_flag:\n",
    "            Z_batch = tf.contrib.layers.batch_norm(Z, center=False, scale=False, is_training=is_training_flag, reuse = reuse_convlayer_flag,scope = layer_name)\n",
    "        else:\n",
    "            Z_batch = Z\n",
    "\n",
    "        # nonlinearity\n",
    "        A = nonlinear_act(Z_batch, name='activation')\n",
    "\n",
    "        # dropout.\n",
    "        hidden_dropout = tf.nn.dropout(A, dropout_prob, name='hidden_dropout')\n",
    "\n",
    "        # pooling.\n",
    "        if is_pooling_flag:\n",
    "            maxpool = pooling_act(hidden_dropout, ksize=M, strides=M_stride, padding='SAME', name='max_pooling')\n",
    "        else:\n",
    "            maxpool = hidden_dropout\n",
    "\n",
    "    return maxpool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "# you should have three X\n",
    "X_1 = tf.placeholder(dtype=tf.float32, shape=[None, 5, 5, 32])\n",
    "X_2 = tf.placeholder(dtype=tf.float32, shape=[None, 5, 5, 32])\n",
    "X_3 = tf.placeholder(dtype=tf.float32, shape=[None, 5, 5, 32])\n",
    "\n",
    "\n",
    "dropout_prob = tf.placeholder(dtype=tf.float32)\n",
    "is_training_flag = tf.placeholder(dtype=tf.bool)\n",
    "F=[5,5,32,64]\n",
    "F_stride=[1,1,1,1]\n",
    "M=[1,2,2,1]\n",
    "M_stride=[1,2,2,1]\n",
    "layer_name = 'L1'\n",
    "output_layer_1 = conv_layer_full(X_1, dropout_prob, is_training_flag, F, F_stride, M, M_stride, layer_name,\n",
    "               is_batch_normalization_flag=False, is_pooling_flag=False,\n",
    "               nonlinear_act=tf.nn.relu, pooling_act=tf.nn.max_pool,\n",
    "               use_cudnn_on_gpu=False, reuse_convlayer_flag = False)\n",
    "output_layer_2 = conv_layer_full(X_2, dropout_prob, is_training_flag, F, F_stride, M, M_stride, layer_name,\n",
    "               is_batch_normalization_flag=False, is_pooling_flag=False,\n",
    "               nonlinear_act=tf.nn.relu, pooling_act=tf.nn.max_pool,\n",
    "               use_cudnn_on_gpu=False, reuse_convlayer_flag = True)\n",
    "output_layer_3 = conv_layer_full(X_3, dropout_prob, is_training_flag, F, F_stride, M, M_stride, layer_name,\n",
    "               is_batch_normalization_flag=False, is_pooling_flag=False,\n",
    "               nonlinear_act=tf.nn.relu, pooling_act=tf.nn.max_pool,\n",
    "               use_cudnn_on_gpu=False, reuse_convlayer_flag = True)\n",
    "\n",
    "encoding1 = my_fully_connected_layer(output_layer_1, 10, 'a', 'b', False)\n",
    "encoding2 = my_fully_connected_layer(output_layer_2, 10, 'a', 'c', True)\n",
    "encoding3 = my_fully_connected_layer(output_layer_3, 10, 'a', 'd', True)\n",
    "\n",
    "loss_triplet = triplet_loss([encoding1, encoding2, encoding3], alpha=0.2)\n",
    "\n",
    "last_layer = tf.contrib.layers.flatten(output_layer_1)\n",
    "logits = tf.contrib.layers.fully_connected(last_layer, 7, activation_fn=None)\n",
    "ground_truth_labels = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "cross_entropy_mean = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=ground_truth_labels)\n",
    "curr = tf.get_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = [1,2, 3]\n",
    "v1, v2, v3 = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = curr.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='a')\n",
    "a = curr.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = 'L1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'a/weight:0' shape=(1600, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'a/biases:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'L1/weight:0' shape=(5, 5, 32, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'L1/bias:0' shape=(64,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_variable = curr.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('train_conv'):\n",
    "    train_step_conv = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy_mean, var_list = a)\n",
    "with tf.name_scope('train_encoding'):\n",
    "    train_step_encoding = tf.train.AdamOptimizer(0.01).minimize(loss_triplet, var_list = b)\n",
    "with tf.name_scope('train_everything'):\n",
    "    train_step_encoding_full = tf.train.AdamOptimizer(0.01).minimize(loss_triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## prepare some training data.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "batch_size = 7\n",
    "input_shape = [5,5,32]\n",
    "\n",
    "X_1_value = np.random.randn(batch_size, input_shape[0], input_shape[1], input_shape[2])\n",
    "X_2_value = np.random.randn(batch_size, input_shape[0], input_shape[1], input_shape[2])\n",
    "X_3_value = np.random.randn(batch_size, input_shape[0], input_shape[1], input_shape[2])\n",
    "\n",
    "dropout_prob_value = 0.5\n",
    "is_training_flag_value = 1\n",
    "ground_truth_labels_value =np.array(range(7))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "initializer = tf.global_variables_initializer()\n",
    "sess.run(initializer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 0.031802\n",
      "B1 0.000000\n",
      "Wa 0.022093\n",
      "Ba 0.061073\n",
      "W_fully_connected -0.021150\n",
      "B_fully_connected 0.000000\n"
     ]
    }
   ],
   "source": [
    "trainable_variable_value = sess.run(trainable_variable)\n",
    "print('W1 %f' % trainable_variable_value[0][0,0,0,0])\n",
    "print('B1 %f' % trainable_variable_value[1][0])\n",
    "print('Wa %f' %  trainable_variable_value[2][0,0])\n",
    "print('Ba %f' %  trainable_variable_value[3][0])\n",
    "print('W_fully_connected %f' %  trainable_variable_value[4][0,0])\n",
    "print('B_fully_connected %f' %  trainable_variable_value[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input_feeding_dict = {X_1: X_1_value, \n",
    "                        dropout_prob: dropout_prob_value, \n",
    "                        is_training_flag: is_training_flag_value,\n",
    "                        ground_truth_labels: ground_truth_labels_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 0.031802\n",
      "B1 0.000000\n",
      "Wa 0.022093\n",
      "Ba 0.061073\n",
      "W_fully_connected -0.021150\n",
      "B_fully_connected 0.000000\n",
      "W1 0.031840\n",
      "B1 0.001077\n",
      "Wa 0.022093\n",
      "Ba 0.061073\n",
      "W_fully_connected -0.021150\n",
      "B_fully_connected 0.000000\n"
     ]
    }
   ],
   "source": [
    "trainable_variable_value = sess.run(trainable_variable)\n",
    "print('W1 %f' % trainable_variable_value[0][0,0,0,0])\n",
    "print('B1 %f' % trainable_variable_value[1][0])\n",
    "print('Wa %f' %  trainable_variable_value[2][0,0])\n",
    "print('Ba %f' %  trainable_variable_value[3][0])\n",
    "print('W_fully_connected %f' %  trainable_variable_value[4][0,0])\n",
    "print('B_fully_connected %f' %  trainable_variable_value[5][0])\n",
    "sess.run(train_step_conv, feed_dict=X_input_feeding_dict)\n",
    "trainable_variable_value = sess.run(trainable_variable)\n",
    "print('W1 %f' % trainable_variable_value[0][0,0,0,0])\n",
    "print('B1 %f' % trainable_variable_value[1][0])\n",
    "print('Wa %f' %  trainable_variable_value[2][0,0])\n",
    "print('Ba %f' %  trainable_variable_value[3][0])\n",
    "print('W_fully_connected %f' %  trainable_variable_value[4][0,0])\n",
    "print('B_fully_connected %f' %  trainable_variable_value[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 0.031840\n",
      "B1 0.001077\n",
      "Wa 0.022093\n",
      "Ba 0.061073\n",
      "W_fully_connected -0.021150\n",
      "B_fully_connected 0.000000\n",
      "W1 0.031840\n",
      "B1 0.001077\n",
      "Wa 0.032093\n",
      "Ba 0.058335\n",
      "W_fully_connected -0.021150\n",
      "B_fully_connected 0.000000\n"
     ]
    }
   ],
   "source": [
    "X_input_feeding_dict = X_input_feeding_dict = {X_1: X_1_value, \n",
    "                                               X_2: X_2_value,\n",
    "                                               X_3: X_3_value,\n",
    "                        dropout_prob: dropout_prob_value, \n",
    "                        is_training_flag: is_training_flag_value,\n",
    "                        ground_truth_labels: ground_truth_labels_value}\n",
    "trainable_variable_value = sess.run(trainable_variable)\n",
    "print('W1 %f' % trainable_variable_value[0][0,0,0,0])\n",
    "print('B1 %f' % trainable_variable_value[1][0])\n",
    "print('Wa %f' %  trainable_variable_value[2][0,0])\n",
    "\n",
    "print('Ba %f' %  trainable_variable_value[3][0])\n",
    "print('W_fully_connected %f' %  trainable_variable_value[4][0,0])\n",
    "print('B_fully_connected %f' %  trainable_variable_value[5][0])\n",
    "sess.run(train_step_encoding, feed_dict=X_input_feeding_dict)\n",
    "trainable_variable_value = sess.run(trainable_variable)\n",
    "print('W1 %f' % trainable_variable_value[0][0,0,0,0])\n",
    "print('B1 %f' % trainable_variable_value[1][0])\n",
    "print('Wa %f' %  trainable_variable_value[2][0,0])\n",
    "print('Ba %f' %  trainable_variable_value[3][0])\n",
    "print('W_fully_connected %f' %  trainable_variable_value[4][0,0])\n",
    "print('B_fully_connected %f' %  trainable_variable_value[5][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Operation 'train_everything/Adam' type=NoOp>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_step_encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle_speechrec]",
   "language": "python",
   "name": "conda-env-kaggle_speechrec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
